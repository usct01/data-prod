# -*- coding: utf-8 -*-
"""Data_prod_insight_updated_@.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7oZ8TYY1TKdR7rXt5N2NJg9A5U26Llk
"""

import pandas as pd
from pathlib import Path

datapath1 = Path('/content/drive/MyDrive/json150')
datapath2 = Path('/content/drive/MyDrive/json_115')

file_list1= list(datapath2.glob('*.json'))
file_list2= list(datapath2.glob('*.json'))
file_list = file_list1 + file_list2
len(file_list),file_list[0]

import json
json_data=[]
good_file=[]
bad_file=[]
for file in file_list:
    try:
        # print(file)
        f = open(file)
        data = json.load(f)
        json_data.append(data)
        good_file.append(file)
    except:
        bad_file.append(file)
    f.close()

print(len(json_data))
# print(len(bad_file))
print(f"Good files: {len(good_file)}, Bad files: {len(bad_file)}")
# json_data

list(bad_file)

##above bad_file do not contain any data

df=pd.json_normalize(json_data)

"""#combining all data to data frame"""

df.head()

df=pd.json_normalize(json_data)
df['Timestamp']=df['Timestamp'].str.replace("_","-",2)
df['Timestamp']=df['Timestamp'].str.replace("_"," ",1)
df['Timestamp']=df['Timestamp'].str.replace("_",":",2)
df['Timestamp']=pd.to_datetime(df['Timestamp'],format='mixed',errors='coerce')
df['Timestamp']=pd.to_datetime(df['Timestamp']).dt.normalize()
df['call_weekday'] = df['Timestamp'].dt.day_name()
df['call_month'] = df['Timestamp'].dt.month_name()
df['CallerContent']=df.loc[:,(df.columns.str.endswith('CallerContent'))].apply(lambda x: ' '.join(x.dropna().astype(str).values),axis=1)
df['ReceiverContent']=df.loc[:,(df.columns.str.endswith('ReceiverContent'))].apply(lambda x: ' '.join(x.dropna().astype(str).values),axis=1)

print(df.isna().sum())
print(df[['CallerContent','ReceiverContent']])
df.sample(10)

df_n1 = pd.DataFrame(columns=['ts'])
df_n1['ts'] = df['CallContent.Seq1.Timestamp']
df_n1

df_n1['ts']=df_n1['ts'].str.replace("_","-",2)
df_n1['ts']=df_n1['ts'].str.replace("_"," ",1)
df_n1['ts']=df_n1['ts'].str.replace("_",":",2)
df_n1['ts']=pd.to_datetime(df_n1['ts'],format='mixed')

df_n1



df_n1['Timezone'] = df_n1['ts'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.hour
df_n1_grouped = df_n1.groupby('Timezone')['ts'].count()
df_n1_grouped_morning = df_n1_grouped[0:9].sum()
df_n1_grouped_afternoon = df_n1_grouped[9:17].sum()
df_n1_grouped_evening = df_n1_grouped[17:24].sum()

print("Total calls in morning:", df_n1_grouped_morning)
print("Total calls in  afternoon:", df_n1_grouped_afternoon)
print("Total calls in evening", df_n1_grouped_evening)



df.info()

df.shape

df.columns

len(df.columns)

df.head()



"""#Number of calls by person  making the calls"""

print(df.Caller.value_counts(ascending = False))

print(df.Receiver.value_counts())

"""#Sentiment analysis on caller content

"""

from transformers import pipeline
sentiment_model = pipeline("sentiment-analysis", truncation=True, max_length=512, model='distilbert/distilbert-base-uncased-finetuned-sst-2-english')

#Sentiment Analysis of Caller Data
df['sent_model']= df['CallerContent'].apply(sentiment_model)
df['Caller_Sentiment']= df['sent_model'].apply(lambda s: s[0]['label'])
df['Caller_Sentiment_Score']= df['sent_model'].apply(lambda s: s[0]['score'])
print(df[['CallerContent','Caller_Sentiment','Caller_Sentiment_Score']])

df.Caller_Sentiment.value_counts()

df.Caller_Sentiment_Score.describe()

##*Emotional analysis of caller content*##
# model= "pysentimiento/robertuito-sentiment-analysis"
model="bhadresh-savani/distilbert-base-uncased-emotion"
emotion_model = pipeline(model=model, truncation=True, max_length=512)

data = ["I love you", "I hate you"]
emotion_model(data)

df['emot_model']= df['CallerContent'].apply(emotion_model)
df['Caller_Emotion']= df['emot_model'].apply(lambda s: s[0]['label'])
df['Caller_Emotion_Score']= df['emot_model'].apply(lambda s: s[0]['score'])
print(df[['CallerContent','Caller_Emotion','Caller_Emotion_Score']])

df.head()

df.describe()

#Calls per weekday
df.groupby(df.call_weekday).count()

df['call_weekday'].unique()

df.call_weekday.value_counts()

df['call_month'].unique()

df.groupby(df.call_month).count()

#day wise total number of calls
df_grouped = df.groupby('Timestamp')
df_grouped = df_grouped.apply(lambda x: x.groupby(pd.Grouper(key='Timestamp', freq='8H')).size())
df_grouped

# groupby timestamp in different months and count total number of calls

df_grouped_months = df.groupby(['call_month','Timestamp']).count()
df_grouped_months





df['CallContent.Seq1.Timestamp'].value_counts()

"""#Sentiment analysis on receiver call

"""



from google.colab import drive
drive.mount('/content/drive')

df['sent_model']= df['ReceiverContent'].apply(sentiment_model)
df['Receiver_Sentiment']= df['sent_model'].apply(lambda s: s[0]['label'])
df['Receiver_Sentiment_Score']= df['sent_model'].apply(lambda s: s[0]['score'])
print(df[['ReceiverContent','Receiver_Sentiment','Receiver_Sentiment_Score']])

"""#Emotional analysis on receiver content"""

# model= "pysentimiento/robertuito-sentiment-analysis"
model="bhadresh-savani/distilbert-base-uncased-emotion"
emotion_model = pipeline(model=model, truncation=True, max_length=512)
emotion_model(data)

# Emotional Analysis of Caller Data
df['emot_model']= df['ReceiverContent'].apply(emotion_model)
df['Receiver_Emotion']= df['emot_model'].apply(lambda s: s[0]['label'])
df['Receiver_Emotion_Score']= df['emot_model'].apply(lambda s: s[0]['score'])
print(df[['ReceiverContent','Receiver_Emotion','Receiver_Emotion_Score']])

df.describe()

df.Receiver_Emotion.value_counts()



"""#length of call based on word count

"""

df.head()

print(df.Receiver.value_counts())

#length of call based on receiver content
df['ReceiverContent']

df['totalwords_receiver'] = df['ReceiverContent'].str.count(' ')
df['totalwords_receiver']

df['totalwords_receiver'].describe()

#length of call based on caller content
df['CallerContent']

df['totalwords_caller'] = df['CallerContent'].str.count(' ')
df['totalwords_caller'].describe()

##Sentiment by person making the call
df.Caller_Emotion.value_counts()

df_n =df[df['Caller_Emotion']=='anger']['Caller']

df_n.value_counts()

df[df['Caller_Emotion']=='sadness']['Caller']

print(df[df['Caller_Emotion']=='sadness']['Caller'].value_counts())

df[df['Receiver_Emotion']=='fear']['Receiver'].value_counts()

df.columns.to_list()



"""##Counting most frequent words in caller and receiver content

"""

from collections import Counter
receiver_content_words = []
for content in df['ReceiverContent']:
  receiver_content_words.extend(content.split())

receiver_content_word_counts = Counter(receiver_content_words)

most_common_words = receiver_content_word_counts.most_common(10)

print("Most common words in receiver content:")
for word, count in most_common_words:
  print(f"{word}: {count}")

import gensim
from gensim.parsing.preprocessing import preprocess_documents

# Remove stopwords from receiver_content using gensim
receiver_content_without_stopwords = []
for content in df['ReceiverContent']:
    processed_content = preprocess_documents([content])
    receiver_content_without_stopwords.append(processed_content[0])

# Determine the most common words
receiver_content_word_counts = Counter(word for content in receiver_content_without_stopwords for word in content)
most_common_words = receiver_content_word_counts.most_common(30)

print("Most common words in receiver content without stopwords:")
for word, count in most_common_words:
    print(f"{word}: {count}")

# Remove stopwords from Caller using gensim
caller_content_without_stopwords = []
for content in df['CallerContent']:
    processed_content = preprocess_documents([content])
    caller_content_without_stopwords.append(processed_content[0])

# Determine the most common words
caller_content_word_counts = Counter(word for content in caller_content_without_stopwords for word in content)
most_common_words = caller_content_word_counts.most_common(30)

print("Most common words in receiver content without stopwords:")
for word, count in most_common_words:
    print(f"{word}: {count}")



"""##length of call

"""

df.head()



df.columns.to_list()

df_n2 = df
df_n2

import pandas as pd

def parse_timestamp(timestamp_str):
    formats = [
        '%Y_%b_%d_%H:%M:%S',
        '%Y_%m_%d_%H:%M:%S',
        '%Y-%m-%d %H:%M:%S',
        '%Y-%b-%d %H:%M:%S','%Y-%m-%d'
    ]
    for fmt in formats:
        try:
            return pd.to_datetime(timestamp_str, format=fmt)
        except ValueError:
            pass
    raise ValueError(f"Failed to parse timestamp: {timestamp_str}")

delta_list = []
for i in range(1, 54):
    seq_timestamp = df_n2[f'CallContent.Seq{i}.Timestamp']
    seq_timestamp = seq_timestamp.fillna('')  # Replace NaN values with empty strings
    seq_timestamp = seq_timestamp.apply(parse_timestamp)
    delta_list.append(seq_timestamp.diff())

delta_df = pd.DataFrame(delta_list).transpose()
delta_df.columns = [f'Delta{i}' for i in range(1, 54)]

df_n2 = pd.concat([df_n2, delta_df], axis=1)

total_delta = delta_df.sum(axis=1)
df_n2['Total Delta'] = total_delta

print(df_n2['Total Delta'])

df_n2.groupby('call_month')['Total Delta'].mean()

"""#Reading Records as rows for each sequence of calls"""

df.columns.str.strip()

df1 = df
df1.head()



id_vars = ['Caller', 'Receiver', 'Timestamp']
value_vars = [col for col in df1.columns if col.startswith('CallContent.Seq')]
df_long = pd.melt(df1, id_vars=id_vars, value_vars=value_vars, var_name='CallSequence', value_name=('CallContent'))
df_long



df_long2 = df1.melt(id_vars=['Caller', 'Receiver', 'Timestamp', 'call_weekday',  'CallerContent', 'ReceiverContent', 'Caller_Emotion', 'Receiver_Emotion'],
                 value_vars=[f'CallContent.Seq{i}.ReceiverContent' for i in range(1, 55)],
                 var_name='Sequence', value_name='ReciverContent')




df_long2

df_long2 = df1.melt(id_vars=['Caller', 'Receiver', 'Timestamp','CallerContent','ReceiverContent'],
                 value_vars=[f'CallContent.Seq{i}.ReceiverContent' for i in range(1, 55)],
                 var_name='Sequence', value_name='ReciverContent')


# Reset index
df_long2.reset_index(drop=True, inplace=True)

df_long2



df.info()



"""#Business Insights"""



#Productivity of call center employee - based on number of calls by employee
##(good and bad files combined)
df.Receiver.value_counts(ascending= False)

"""#Streamlit"""

!pip install -q streamlit

!npm install localtunnel

!wget -q -O - ipv4.icanhazip.com

df_long3 = df1.melt(id_vars=['Caller', 'Receiver', 'Timestamp', 'call_weekday',  'CallerContent', 'ReceiverContent', 'Caller_Emotion', 'Receiver_Emotion'],
                 value_vars=[f'CallContent.Seq{i}.ReceiverContent' for i in range(1, 55)],
                 var_name='Sequence', value_name='ReciverContent')
df_long3.columns



df_long3.head()

df_long3.to_json('df_long3.json')
df_long3



# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# 
# 
# df_long3 = pd.read_json('df_long3.json')
# st.set_page_config(page_title="Data Prod", layout="wide", initial_sidebar_state="collapsed")
# 
# st.write("# Customers Call Data")
# 
# # Sidebar for employee name and color theme selection
# with st.sidebar:
#     st.title('🏂 Employ Name')
#     st.write('Select a color theme:')
#     color_theme_list = ['Blues', 'Cividis', 'Greens', 'Inferno', 'Magma', 'Plasma', 'Reds', 'Rainbow', 'Turbo', 'Viridis']
#     selected_color_theme = st.selectbox('Select a color theme', color_theme_list)
# 
#     input_caller = st.text_input("Enter a caller name")
#     input_receiver = st.text_input("Enter a receiver name")
# 
# # Center output for caller name
# if input_caller:
#     # Filtering the data
#     filtered_df_caller = df_long3[df_long3['Caller'].str.contains(input_caller, case=False, na=False)]
# 
#     if not filtered_df_caller.empty:
#         st.write("### Caller Statistics")
#         st.write(filtered_df_caller['Receiver'].value_counts(ascending=False))
# 
#         # Displaying the records
#         st.write("Caller Emotions, Receivers, Timestamp, and Day")
#         st.write(filtered_df_caller[['Caller_Emotion', 'Receiver', 'Timestamp', 'call_weekday', 'CallerContent']])
# 
#         # Creating a heatmap
#         st.write("### Heatmap between Caller Emotion and Receiver")
#         heatmap_data = filtered_df_caller.pivot_table(index='Caller_Emotion', columns='Receiver', aggfunc='size', fill_value=0)
#         fig, ax = plt.subplots()
#         sns.heatmap(heatmap_data, cmap=selected_color_theme, ax=ax)
#         st.pyplot(fig)
# 
#         # Creating a donut chart for call_weekday
#         st.write("### Donut Chart of Call Weekday for Caller")
#         call_weekday_counts = filtered_df_caller['call_weekday'].value_counts()
#         fig, ax = plt.subplots()
#         ax.pie(call_weekday_counts, labels=call_weekday_counts.index, autopct='%1.1f%%', startangle=90, wedgeprops=dict(width=0.3))
#         ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
#         st.pyplot(fig)
#     else:
#         st.write("No records found for the entered caller name.")
# 
# # output for receiver name
# if input_receiver:
# 
#     filtered_df_receiver = df_long3[df_long3['Receiver'].str.contains(input_receiver, case=False, na=False)]
# 
#     if not filtered_df_receiver.empty:
#         st.write("### Receiver Statistics")
# 
#         # Displaying the records
#         st.write("Receiver Emotions, Callers, Timestamp, and Day")
#         st.write(filtered_df_receiver[['Receiver_Emotion', 'Caller', 'Timestamp', 'call_weekday', 'ReceiverContent']])
# 
#         # Creating a heatmap
#         st.write("### Heatmap between Receiver Emotion and Receiver")
#         heatmap_data = filtered_df_receiver.pivot_table(index='Receiver_Emotion', columns='Receiver', aggfunc='size', fill_value=0)
#         fig, ax = plt.subplots()
#         sns.heatmap(heatmap_data, cmap=selected_color_theme, ax=ax)
#         st.pyplot(fig)
# 
#         # Creating a donut chart for call_weekday
#         st.write("### Donut Chart of Call Weekday for Receiver")
#         call_weekday_counts = filtered_df_receiver['call_weekday'].value_counts()
#         fig, ax = plt.subplots()
#         ax.pie(call_weekday_counts, labels=call_weekday_counts.index, autopct='%1.1f%%', startangle=90, wedgeprops=dict(width=0.3))
#         ax.axis('equal')
#         st.pyplot(fig)
#     else:
#         st.write("No records found for the entered receiver name.")
#

!streamlit run app.py & npx localtunnel --port 8501

!wget -q -O - https://loca.lt/mytunnelpassword

df_long3.to_json('df_long3.json')
df_long3

!streamlit run app.py & npx localtunnel --port 8501

!streamlit run app.py & npx localtunnel --port 8501

